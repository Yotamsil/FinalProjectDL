{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx3i2Op-6X5n"
      },
      "source": [
        "# **Introduction to Deep Learning: Final Project**\n",
        "\n",
        "**Submitted by:**\n",
        "\n",
        "Roei Matz       205871478\n",
        "\n",
        "Yotam Silverman 313532418"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Ki8M-pCmgF"
      },
      "source": [
        "# Project Description\n",
        "\n",
        "The data for this project was gathered from the [sign language MNIST](https://www.kaggle.com/datamunge/sign-language-mnist) dataset from the website kaggle.com.\n",
        "\n",
        "\n",
        "The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2â€¦.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255.\n",
        "\n",
        "Our project's objective is to design and build a neural network that will identify the letters given in each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zdEvcdO6X5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74799ff-cf1a-4468-cec5-6151fccd5734"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# CapsNet related imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "# clone package repository\n",
        "!git clone https://github.com/jindongwang/Pytorch-CapsuleNet.git\n",
        "\n",
        "# navigate to directory\n",
        "%cd Pytorch-CapsuleNet\n",
        "\n",
        "# get modifications made on the repo\n",
        "!git pull origin master\n",
        "\n",
        "# import it\n",
        "from capsnet import CapsNet\n",
        "#\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pytorch-CapsuleNet'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 57 (delta 1), reused 3 (delta 1), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "/content/Pytorch-CapsuleNet\n",
            "From https://github.com/jindongwang/Pytorch-CapsuleNet\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70FvfUt8E0aX"
      },
      "source": [
        "Mount Google Drive and load the project's data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRlPFrSaEoJi",
        "outputId": "ab94633f-b766-4ad8-8083-cbaac18e0191"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "train_csv = open('/content/gdrive/My Drive/Intro_to_Deep_Learning/sign_mnist_train.csv')\n",
        "test_csv  = open('/content/gdrive/My Drive/Intro_to_Deep_Learning/sign_mnist_test.csv')\n",
        "x_train_val = np.genfromtxt(train_csv, delimiter=',')[1:,1:]\n",
        "x_test = np.genfromtxt(test_csv, delimiter=',')[1:,1:]\n",
        "\n",
        "train_csv = open('/content/gdrive/My Drive/Intro_to_Deep_Learning/sign_mnist_train.csv')\n",
        "test_csv  = open('/content/gdrive/My Drive/Intro_to_Deep_Learning/sign_mnist_test.csv')\n",
        "t_train_val = np.genfromtxt(train_csv, delimiter=',')[1:,0]\n",
        "t_test =  np.genfromtxt(test_csv, delimiter=',')[1:,0]\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the train set to train and validation"
      ],
      "metadata": {
        "id": "SwPEZ6Xpc77u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train_val[round(0.8*len(x_train_val)):]\n",
        "x_train = x_train_val[0:round(0.8*len(x_train_val))]\n",
        "\n",
        "t_val = t_train_val[round(0.8*len(t_train_val)):]\n",
        "t_train = t_train_val[0:round(0.8*len(t_train_val))]"
      ],
      "metadata": {
        "id": "7PBs_xGAdAt1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo6PrBdGsC4M"
      },
      "source": [
        "Here we show few examples of the letters notions in the sign language:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "PzC3fRTiKEZn",
        "outputId": "b4852b4b-e332-4e3a-fc43-fbdf852b4d79"
      },
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(6, 3))\n",
        "for img in x_train: \n",
        "  plt.subplot(1, 5, i + 1)\n",
        "  img = np.reshape(img, (28, 28))\n",
        "  plt.imshow(img, cmap='Greys_r')\n",
        "  plt.axis('off');\n",
        "  i += 1\n",
        "  if i == 5:\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29W29jR3YFvHi/30WKal27pZ6+GR5M7DGSgZMAmQEGCOYH5S/kn+Q1QN6CPAXJTN7i2LFj99jt1l2iSPF+J8/3IKzdm6U65CEl+/uAjxsQJJE8h1X7VO1ae+1du3yO42Ata1nLWtby84j//+0GrGUta1nL/59kbXTXspa1rOVnlLXRXcta1rKWn1HWRncta1nLWn5GWRvdtaxlLWv5GWVtdNeylrWs5WeU4Lw3/+Ef/sEJBAJIpVIIhUKIx+MIhUKIxWIIBoMIhULy2+/3IxgMwu/3IxAIwO/3y98+nw9+v3/mN//m+z6fDwAwGAwAAJFI5N5ntehrTOHrOh3OcRy4pcc9e/bMfiOL/Pu//7uj2+N2T7MNZh/YHvbDcRxMp1P5bYpuP/82+6TvoT+n76vbM5lMMBwOcXNzg//+7/+G4zjI5XIIBAL4x3/8R886AYD/+7//k4bY9G/2RT879l+/v+h622u6f3zNyz3evn2LP/3pTzg7O8N//Md/IBgMIp1OIxKJIJVKwe/3IxaLAQD+5V/+xbNeLi8vPedjmjrR7dP6NHVj6to2RszvMe9lfp5tCQQCGAwGaDab+Oabb/BP//RP6PV6Mo75+9/+7d886+TLL7+U+aPnt57n+j32ka9pnWgbYLMH5v+mziiTyUT6Y96Xouekl+/KZDKuOplrdN0aQDEN4SIxlWS+N51OMRqN4DgOgsG7ptEo/39FbO32YnhtfXB7zWa0bEbc9r1+v1+MD6/x+/2YTCb3DJ1eCEOhEIC7xS4QCLj0fjlxa+NjPU+boXb7rnn56IFAAMFgEIFAQAADr5tOp1b9rSpe2jPvWtuYeMg9vMhkMsFkMplZvP1+/0p90O3gb3NcLurXIoM37zrbQsO+0Z5xLJjteiyZa3SJYDkYiWT1jw3d2lYh83WNcH0+H4LBIPr9Pt6+fYvhcIhnz54hHo8jnU7fM+7mw16EeN1klUFDg6QR6jLf6fY5rS89oE3kq7+Tf9vQj27XdDq1TvbJZIJQKIREIoHNzU0Eg0Hs7OyIAV5FzEVjnqFYZCjN/lE4HsbjsUwavm5D0IsWvWQyiXK5jH6/j3w+DwCIRqPiDUwmE/T7/eUU4dIf3Q7T89Btngd0Fi1kNkRnewa2Z8Oxxvne7XbRbDbRbDYxGAwwHA4RDAYxmUxcvbJ5YkO35o9uo/682fZ5KNdNN/qzw+EQ4/EYjUYD3W4X4/EYAFAsFpHNZu99xyK9e537c42uNqL69zyxKdDtPfPHcRw0Gg0Mh0MMh0OEw+G5btcyHTU/t+oqbXv4i77Li8xzuc3+L3t/PVFNg8jJFY1GxQCHw+Gl22/Th5sBsb3u9nxseqBxoEEcDodwHAfhcFgWRf18F41ZUmfRaBSRSATT6VTuQ4M+nU6XNjBmm72MFS9IeBm0qnVtGljb67Zr+/2+eKB6EaRulpFlAJKbsZz3+jz7YF7PvgwGA3S7XQwGA4xGI6TTafFsbHOH165icAGPRtdEshrxmgbZywo2T8GtVgu9Xg+tVguBQACZTMY66VYxbLbvW1a8Ljpe3cB5xoX/a/5Yows3Xk5fy7/Ne0wmE3mmkUgE8XhcqAmu+MuIF6PiZcGwDXA9vhzHwXg8xnA4xPn5OZrNJn788UeMRiPs7+8jmUxKH9mPvb09ZDIZMZy6HT6fD/F4HLu7u2i328jlcjL5JpMJxuMxJpMJBoPB0gu1DXmb79uEY2EefWIiXjfjafZVjw/zdY4TbUyHwyEuLy/RbDYRCoVEFwBWoqHMObysrTBfs93PbU7Qu6Ynx8W61+uhXq/j+++/R61Wk7hVPB6fS2/aFgEvY2QpTtckt00l2BDuMgbScRwMh0OMRiMZ9KZ76OVeXpWwiuFddI3uL9u9iqFZ9P02isE26MzPmPehMePgeiiie4zPzOsLjelgMMDt7S3q9TouLy8xHA4Rj8fFOBIJO46Dcrks97Y9m0AggEQiIUh3MpkIsqPRHY1GK+vEbXK6Lc5uNJGbrmy6099h3lO3Yd53A3dottvtYjgcyvzn+FgVuNiMq+0z5t82A+x2f+BD8IuLCMc6MGvLuJA0m01Uq1XxtsPhsGsAbRl7ZMpco0tES2JZI17yvG6r06IVjcLOc2D3ej30ej0htt2u8yo2BElZhWJggG9VmsO81ja49IM20avjOAgEAvde09fZEJLmddkPGif9d6PRWDpAqsXNWJrtmTdh+P2BQECCq6PRCO12G4PBACcnJ2g2mzg+Pkan08HJyYm4iOFwGOFwGH6/H6PRCNPpFKlUCuFwGNFoVCgrrQ/SK7FYDOFwGL1eD81mE6PRCN1ud6Zfy+rCra9eJ61pDM3f8/S4CHyY44Q6IYKdTqcYDAaoVCqo1+sYDAYYj8fiEfX7/ZU4Xd1u/b/NKLrNf/1Zm73hgjkej1Gv1zEejwXllkolhEIhWVDp/fB5X15eolgsSnYW7V0sFpNMLnL+q8hSSHeeAZyHgM372f6ny0ty20TVq3aQ16/K4druZbvnQ9pnXm/+rQ0rkYZbO2zoRl9rTkSNWqbTKYbD4YP64OVZ2Z6FnoQmnUDDy6DO+fk52u02bm5u0Ov1UKvVxFWMRCKIRqMIBAIYDoeYTqeo1WrodrsIBALCV5sLfzAYnOGEGTSiKx0Oh1c2uo8h5jNd9Dpfm9cGm1E2x9NkMkGv15sxsLxuPB4vbXTd0KIbsnVDxfOQsgYv4/EYnU5nJisqm83Kgq5BCY10t9tFvV6fAZnBYBCpVEpAF1MI5+nfTTwbXTcOV6fZ6HQb/dvMauC9/X4/wuEwJpMJOp2OPNzRaDQzeZYZvPMm/UPQqe0aE0W7IQvzdRvi1iu0LVvDpAds7+vgD42V5oUjkQgSiYRwWN1uF9VqFX6/H7lcDr1eT/jRh+jF1mf9urlg0KvRqIJ8arPZxNu3b1Gv13FycoJut4vr62sMh0Pc3t6KW0gXMhQKIRQKIRAICNL94osvcHp6is8++wwvXryQ8ah1Ew6HkclkUC6XMZ1O8eWXX2IwGGAwGMyklK2ik2XoI3N86HtwbJif0Ysrnz37rj9jo5h0rEbPUd6Li1q325V0Qhqnfr+/Utoof5sG1bQdbteYf5v9CYVC96ihbreLm5sbBAIB5PN50ZFOh9PI+OzsDMlkEul0Gs1mE/V6Xbjew8NDvHnzRmyfbqMX8WR03ZS1KHDmhlbNz/AB040cj8cC7d0Q4LKyyoq06H4ahdoGge1122f0/WxtNK+xpQ2ZbdErOCcG3SJyot1uF41GA5FIBNlsVvg7ortl9WFrq9trpuiURNInnPTHx8eo1+t4//49+v0+arWa0A06yMUFmws1DdDx8TGq1SoODw/vxQhoyHw+H6LRKFKpFCKRCPr9vgTUOIkf21vi98/7nLmwu3HBpuGl0eXrOj6ixyUXKKaHArO53sPhUHShF3UvHs0iHcxDrbbxtMjg8jdT2jQHzUUaAHq9noA9xjD4w/TAdrstgcN2u41KpSK6i0ajsnib+vcinrMXzCwGk891ewA2lKzf05HEwWAgSmNQw8bbrCqPYXhNTmpe22yDySZ6wuiFyLyXiVZMDpfv0e0LBoOCToLBIMbjMSqVCq6urvA///M/kh0Sj8eRyWTg9/uRSCQexOl6Fa0PPabIFbbbbVxeXuL4+Bhv374VDpfphBrJMAbAVC+TCuj1enCcu9QgfkZTF/xsMpnE4eEh+v0+gsGg0BOMNayql3mGRIteDNzQrtt1+m/tLZm7EYfDoXgRGglzZ2ksFkMymQRw91zIdTuOg263i9FohE6nM2OwVtHHPDBnIl/zGttzIFLVHC31wP4wMKbTDUk/tFotGRulUgkvX74Ujr/T6cBxHHQ6HTQaDZRKJbRaLdlHYPZtkXhCul5SwWxfaFOoFrqUVALdWr6+bJK+G4Iw0cJDxG3i2P43ka7tu23IxeyDnnTzUJKeXBxANLqhUAjNZhO3t7c4Pz/H8fEx0uk08vk8IpGIRO3NiK1XmbfozhOOLbpqHAftdhsnJyc4Pz/HxcUFGo0Gzs/PBbnzOhoSTjCiMb39nHwe0Z5ul/4dDoexubmJ09PTmXxfGt5lx46Xz7t5PvM+axpZ2/V68bbxlr1eTzKFptMpQqEQwuEwcrkcIpHITBA9HA7PLECDwWBGl8vIvHmj7cQ8isEmpAvYP44p0mr0ntkPGl16SuSs/X4/MpkMNjc3pR3RaBTAHTAk1dDv98U+PSrSNWsomNsk5yE8PZl4D/2ediO5K+T29laQro4czlO2m8thM7oPTXUB7udezjPmbrSDrf26feb9TCOoJ5LjOOIiUq9c8ekqETlWq1VcXFxgNBpha2sLmUwG29vbiEQiwgnv7u4+KJhm9s1tfGgeWo+rTqeDH374AWdnZ/jmm28kLazVagkHx34yqMHgGO9Do5FMJhGNRrG7u4t8Po+nT5/OBMu0cJFKJpPY2trC559/jtvbW1xcXAgVs0rQaBWxGVe9iLvxsxQuRNPpVPrb7/dnqIJYLIZEIoF+vy+egv4djUaRSCTQbrcRjUbFS9Ao8qGL8yJQZr7nBmB8Pt9Mdkuj0QDwoX5LKBSCz/chHZLjiJkupI9SqRQAoFwuI5fLzehGL/I6HdGLJ2vKQqPrhnS9KtE0uPyhQSUC4cTie2ZKGq+fF5QxH5b5/zKowk3mJbS7vbZoJTQ/p/VkBsTYbu3aEY1QbzS47XYbvV4PNzc3OD09FZSbyWSws7ODfD6PfD4v3xMIBLC5ubnSZJqnj3kD0uQTO50Ovv32W1xcXODdu3dot9toNBozu6L0biFOKt6LiDUUCgl1cnh4iGfPnmFra+vebjttxIhqSqUSjo6O0Gq1kEqlZAPGsptGbMZz0efdPDTzf9Pw6oWEhmU6nUowWmeADAYDpFIp2UjCxXk4HEpwNZPJoFAoSBqeTit86PhwQ7f6PRuYcpvjNKidTkeK8ziOg3Q6jUQigf39fdGLRrh6k5fjOFLIK5/PI5lMSoBf0zVa7zZP1IvMNboUG4drZirY/nabbEQpPp9PgjrMBYzH40gmkzIRzbw9dm6R62FTgOluPYZ4MS6LVkEbStfGVuuAv+kqah6X/3c6HUl7ocGaTqdIp9Pys7m5iVgsNjOYAMiq/xA9LNN3Gk2di1ur1WS/P2sekF/UPCzHEHNw37x5g0wmg0wmg0gkgnQ6jWg0ip2dHTHA89pBXYdCIUQiEfh8PiSTSYzHYzx79mylba/zdOA2RvX7i8Y07+/z+QS89Ho9vH37FoPBAOVyGdFoFDc3NzImyOuWy2VxxTnX+v0+6vU6arUazs/P5VlwgwR3MurA3Cq6sGUzmTrQ75uxILZZX0M7FQqFMBgMcHFxgVgsJsCCfC+NrvaqdcEnelAAZrhfPg8Ncmx9WyQLja4N3tsoBhu6NV1xfc9QKCTJ1/1+XwZFIpFAOp0Wt8gNYXtptynz+NOH3Hfe68tcZ+7314Vc9EJFWoYuHz/LHOfLy0tcX1+j1Wqh1WpJ1kI+n8fOzo4ES8iDaX0Q0awq8xC/Fo0umd7V7/fRbDZRq9VQr9fRbDYFXTI5na4zXyM6TafT+Pjjj7G3t4dSqSQBIC7eREMaKZrto245qeLxuHB7j0W5mHpZ5AGZhtdEmXpeTCYTNJtNVCoVfPHFF+h2u3jy5AlisRg6nQ6Gw6EswrFYDJFIZGYzSTAYxGAwQLVaFe6W1BSv13UpzLGzbN/Nv222BLhvcPmb80CPIxrdXq+H6+trRCIR7O7uIhwOz9SPYD80lUlPkcaUBpqARi/2bjSVF1kqe8HNCC5yAfSqZBpndpYohIVH2FGdC2fKPENnGwy2QbysmMjQS1vcxKQVTJeFXJTmk+gKjsdjtFotQbEaeQwGA0SjUUSjURSLRUQiEcRiMeHx9K46Pgv+bVIZy/bH7flTeG8zG4bpauRObZwlET5wh8iLxSISiQQODw+RzWbx/PlzFAqFmXrP5th04+CJhBidr1arSCaTSCaTKwcX9b21uHGxbu+7IUpmetAw9Ho9Qafk8q+vr2WjCHNrJ5MJLi8vMZlMEI/HEY/HJSvk9vYW3W53Ju2Mz4lpVkzJMtu6rD7cjO083ZmLDNvgOHcbY4bDIWKxmHhIjBdFo1GZuycnJ4hGo+h0OgiFQvjhhx9wdXWFVCqFeDwuRpXBRY5Pzkm+Zwv0e7EDC42u5lb1JJmHdG2KNF/XxiscDkuqUiKRQDweF+TGyKHNwOuOuhlBG/cy77pF4hXRLrqvuRDRoPJan88nD5wrOidMr9fDYDDA8fExWq2WRJO5YnMiFQoFpNNpGSg6EEIjot3qhy5INj246dxcyMfjsSAqs42mzrhYM+Dx+eefo1wuo1wuS1AQsFfBMt138/XRaIRms4nr62tMp1OJ5j9mGp1t8Z/H5eqxoWUymaDRaGAwGKBer6PdbuP4+Bjtdhvdble2M5tlGP1+P87OznB7e4tMJoNUKiUgp91uo91uz4AATbnoe60KNBbZC/2/7Xpy1Jpjp9Edj8eIxWKyfZubHXSd6B9++AGj0Qjv3r1DMBjE6ekpms0mtre3EY1Gxegyq4cbQgAIlWXSEMvowlP2AmAPIHk1tObnTdohGAziyZMnGAwGcjpFo9FAp9ORZPVEImFNUrdlN8wzwObnlh04Xo2u1/uYxhb4EBAySxdyYpil6FhbgGUZ6Tbq1B8bgrW5eKuK+RzcJgx1rt049rHdbqPZbMruREaZgdmcZGYZHBwcoFQq4cmTJ0in0/dqUngVc4EOBoPiGdCN5EaMZWSeThcBABsg4IKjU51ub2/RbrdxenoqQVPWSKCQRtIbZTimAMy43dS7/k7uDt3Y2JCFezAY4PT0dGnaxYuX7MX4csODGf/QP2wrtwHzFBAu7icnJ5LpQ11wYen3++h2u2i1WhLk5/s0zG706SJZOXvBZkAXrVrmdXwvHA5LoCIYDGI0GuGbb75Br9dDuVyWCWaD89pwmghCu8/8f5Frt0i8TiTbJDKFg4IThHphPzl5mNzPnOZWqyWZCaPRCKVSCfl8XhYofpe520a3RbfPbWAvI7YxME+IJjh5Wq0Wrq6upMoTC0vrdnOsMZ/0s88+w5MnT1Aul6WAienZePE4TAmFQshms8hms4jFYphMJqhUKj8Jr2tri24zjQnHBjn34XCITqeDd+/e4fb2Fu/evZvJNzUND8cSg6f0KHq93gyXqfNcgbti7vF4HLlcDoeHh5L3TAqDiHLZPvJv047osejG5fp8vpkUN7P2LV9jxkatVkMkEkEymYTP50O9Xsf19TX++Mc/4ubmBs+fP8fW1hZevHiBVCol45Epg9fX16jVakgmkygUChLoJ2hYdu4sDKSZEUIvq5SpZDdjrF/janxzc4N+vy+rU61WQ6/Xk4CAuWVPczW8nxudYL72UwTUvHzGNLYmt8oosWl0uCqT1C8UCvD5fMjlcsLVug1qG8Ln9z1ED7b+elmYNGrTe+CBD/UTotHozM4nRs4ZLGEE2bbomouv/m63/tKoJRIJbGxsIB6PS1Dp6upqpZoUjyF6fz89I+7g1BF1rUfdx0Dg7pxDn88nCwnfT6VSSCQSYnQ596grepnM6QaAfr+PQCCAbDaLSCSydH9sIMyL2JAlFxSOC43wOWaop+FwOBMo5WeoI24DByA5ze12WwpwRaNR5HI52b25KljxXPCGK8kiTtd2HV+zKY73m06n6Ha7+OMf/4hut4tMJoNgMIjb21uEQiE8f/5c0nem06kkQhcKhXuVo2yTy4aIH8vY2L7T9jA4YThpiDL05gYGyIbDIer1uuQNBgIBOVKE3O6zZ8+QTCYlcKS3duqAle4z33sM1G/rv9trZnuIrhglZ6DH5/PJRE+n01KwnNs1ifi5xdkt79btWduMMoW0Qi6Xw9HREbrdrnhc5+fnKxV3d9ONbezpz2n+lYuB3rnZbrfR7/eFc2U63e3t7UxFMC5MqVQK0WgUz549Q6FQkO9/+vQp8vm8fE+r1cLNzY20hwtcPB5HsViU4F06ncbJyQl6vd7SuljF4GrvWM8xzpdAIIB4PH4vjZJFi1hPg/ONYI0LVigUQiqVQi6Xg8/nQ6fTwe3trRRVCofDyGazODg4wMbGxsxWcvPZLZKFgTQN/c2buxlZmwGmweaE0as2fzgBB4OBGCRyeHSd6D6w7i7PMjLbpicX3zONzEMNjakDm2hUa55GoIuS+Hw+cQEZPdZoVBdiZp/j8fi9+hRmu2x6sbV3EQp8LH2YC7ZOy2FKEgCJluuYwmg0EvqFE4dUzDw+VPfNZnBNVEh6IRAIoFqtArhDfA/Zlu7lPXNh0EI9sRJft9tFp9ORMaMXNZ3tw9c3NjaQzWZxeHiIjY0NMUzZbFbcbj1XKXw22ithDjULwvwUYo4jt/lKBKvjPWb8gu3WOcnBYBCZTAaDwQC7u7t48uQJUqnUTDpsLBZDOp2WTJBYLCYAQAcTl7UjnrIXzCwGrRhzEun/9W+uJvF4fAZ9jEYjSVdh55iIzdqXfr9ftggTFVarVdzc3Eji87yHZnMvVwmi6Xvo/20GTveP+ae3t7fCmdFocJKTTiCSDQQCyOVysl8c+LBVdXd3V4JkPt+HpG8bwuNz1IPRtjjpVXsVMQOtpp54b3OLNycNt1yyrxq9RaNRWaAYROMmmlgsds/Imt9v4/nN94gYfT6fBG0jkQhOTk6kJsNj0zDm/Uw+Wi9M1BEDZ2dnZxJ4JB1FLwD4ECijYfjoo49wdHSEg4MD8SBs9RNSqRRKpZKAhW63i0qlAgASgPrf//1fMXTmBgEvYvM8tG5sKFhn8ehrGYDleNCLOIWBQsZG/H4/YrEYnj59imKxiF/96lc4ODiQfF5+bzablZKf2WxWMiHa7TZGo5EASP1MvchCekErwvaem7JMhKtzTTkYiP5IXDebTbmXDi5pN5R77xuNhhSe0PvxKW6u82MhObf/531OF1HRrpI2juSbdDqVmZ7D4ivUkRu/5IbubahwEUJ9qPC++jlpfpLbfEmR2IwoES6Nri2He96zti0Ebvogok6n0zLuHtp32+u28WhScNQT58tgMECr1UKn05Htu1x4zfmXyWSQTCZRKpWQyWQErfMZzMuJph6YosU2kCNNJpOPmkq3rLDdeteYjdPm53QlNZ/Ph42NDWQyGWxtbWF3d1cWW45BnWIJQLxQUpsA7sVRvIjnbcA2g0pxQ7lEcfwNQAYwJxhPA3j79i1arZakqLRarRlEyHJy5H+///57XF1d4dmzZ5KfqUl924C2UQ7Lyjy3md+h7083mZXm9fZd9sfnu+OVWCZOTx5yU8CH3WK1Wk24Ncdx8OTJEwmGcILY+qjvaWYzrJp36VU/fN3MGe73+2i1Wmg0GsJTckuwOejJ9ZbLZdl1po8Dd0Ow89pn83g4fiORCJ4+fToTjHkMXWixpfORhtOGhIGzbrcrNTQ4d0xPh2MxHA7j448/xvb2No6OjpDL5QBgpl4yv9e20GlUyE0jpDcikQiePXu20snRpo60PXEDenrscvyalcLa7bZ4zaTnAEhWVDabRTqdlk0UpVIJfr8fv/nNb1Aul+Ve3H7ObdPMFGo2m7i5uUE6ncbLly+RTqddPa15spRvYDO6tr81f6urSGl3h8KO8mHq1YquMwt1UHnAh+hio9FAs9mU/fZmeyluHN+yYrvWNrk4iNl+jW4ZFDF1Zq6aJg2iaQIdcMtkMrJN1qSAbP032zyPD31M0X3VGQlEcBrpakOidcUFXG/vdfsuk1aY9zm393QNkIfoZR6lwN/Ujf6smfandUPawQyeBgIByb7Y2dnBkydPhMrSKNA0arYxQp0zc4QgSKcmrqKLed6z19f5GscTxxBf5wLP63jq9Xg8FnqOC3kkEpFt9RrscLFjsJdxmYfsUPRkdE1jq3leGwrmgyHnoQcteTkTyZGc5357Bk1YXLjT6SAQuCu6zcMDB4MBvvvuO3Q6HXzyySeSFmOKG5pZRXF6ANjuy4HLB9XtdoWj1elOJu/EnWR+v1/Qg04p024UcLdYnZ6eol6vYzKZoFQqYX9/H/F4fGZTgWmw9Wu8p37N/HtZvbgJxwB1QETCoKlGUcy/tKVo0SvQRtftWbCPXl7Xxo/fo9vLLcWriE23HH/aMyJHqtGcifo5Zgk+dB1Y3jOZTOL3v/89tra2sLe3J1vs2WdzDM/TBw2TphtevHgh6PKhmyNsenJ7zw39hsNhsSEEOMzlDofDaDabCATujukpFovIZrMAgEwmI3U7dFYW+89jrWiXer2egAK3Bd/LGPFU8Aa4X/nHTQHmpNDcEREa0Z8eTPysyaXoo59p0DVi7vV6ktZBZS1CP48lbt+jXR+eOmCeLGrm6QKQWhPmLiwdQNDtp9EyeVDb4HVDWrYJ+Fii76mpJ4rm4kwqYd6CaCv7uagNbsje1lb+T8O7qsxrm36e5pzS72mPiUFYs9CMng/RaFQ43Hw+L+mE1K8NfCzSh352TM3SHtxj6cT2/iKvjQui43zYJs8fgj7qkv2gR8jCWtrD1PaHNkrbKupA79b10i8tnrYB6wFoDhD9ZYTzqVRKdvKQeKcrzCM/9DbE4XCIfD6PYDAoZ85zBedhcr1eD5PJBPv7+8hms5hOp4jH44KQ6vW6RBTnuYI/FaLThlO7y41GA5VKRR4ktxHqCTUej+WIIp3hwQecTCZnEuQ5IHg9XSQdSTUHhJ5AWjdc6FZFcW6iF1sAM6mCfJ3jgIhJG129pZftZ0ZLIpFAIpGwLvr6Wdj6v0j0PYnyJpOJ7OB6iOj26fboQy+5GPOHfC6Pijk7O5OMBcdxJI2p0WjIoYnb29s4PDxEKpWa8Y7MQLOJ7m3tZUCRiwO5UWbbPIbRXbR42t5nu4hUe72eZBVwYxWvZaoYYyiMB/B0X21kNZ3AtFR6qz6fT47o0b7lB5oAACAASURBVHzusoDFc/aCF4XxIWnU6jgfzpNn6hRJb5/PJ6Q2C0nQsOtNBKQZ9O4lGiTNcZntWsTj/RRG14ZINQ+rE935OY0Y+LdG9pz4NLhMrdMJ8GZRlnnIzobqbK+vKqYx1ChDGx7N6ZpI17wPr3EcZwbpun3/PA5b3898zxwX88bYIlk0VoDZXHg9Johu9dghgNHZCroP3EhTKpWkUIv2JG3tW/S8NeAy7/HQMqC8v+1/LzaHbdJlGZlPbFIxnEf8TYDDzChgdvOSPvqHz5/fwypj88bgPFkpkMa/TeSrsxS4cnBnx8nJCa6vr/H999+j1Wrh4uIC4XBYyvPt7+8LqnMcB9VqVTgUALLHmwEjM/DAdA+3h0Slmq8tK4soBbaRgcRUKjVjYCjaMOugCI8U1wOm0WgIih2Px7i4uJBVOBAIYGtrC1tbW9ZIONtsMzBaiIBXNbxuqJP/61J4zF1mJSzN5+riK1oYbB2PxzMLtE5FdGuP2wLD98xMDrMfmg5bRczxpzlcTZOYEXkNPNrttlSU04iMhVjS6TRKpRL+6q/+Crlcbqb8oqYGbWJ6P6a+6HVo1M//VxWbx+xmaPXr5vPRNTyY1844B5/X/v4+0uk0Dg8PkcvlkEwmZ7wLGluOS26FZtyIGyJYGF+fI2e20Yt4Nrpmx23/mwOdhldXo7+5uUGj0cC7d+8QjUZFWc+ePZMHTOOhka5Or9KkN6tA0XVdBu4/1KXW32OiFG1kdbuJWjQFoHMDOQB05J4n0RIVsxgMgHvnPM3rt834msGVh+jCJuwjuWyiN521QOOi0a42hBr1aQOo++u22JoLif7N79AoVt/PRFQP1ZHZLp3wr/tqZipoykqjbhpfHYVPp9Oyw8yLbua1T19LA2ci8VX7/1DRY1anp9IecDELBO7OzOM2aF15z83rYpaCmTdOPptFg1btx0J6QUfd+Zqt82YnSGyT0zw+Psb19bUg3m+//RaBQAAnJyc4OjrCJ598Im633jNNfpPkt1aY3+9HKpVCOp2e4UG9ussPcY3mPbDJZIJqtSoFobl7h1FWcrvxeFxOKGBbSR+QkiECZkCu1+vh+++/R7fbxaeffoonT55IHVGKnsxufB55XFNHD02NotCIMYDIhZIDWWctMD+XiEKjPL0I+Xx3e+K//vpr3N7e4le/+tXMGLXRE9rwateROuVvBiRHo5HEJcLhMNLptLjtD/UAKBzDeju8NrJ6LJFT1MhL/3Q6HTnzjvEO5tXOM7qaRjEXXo26+T49zGAwKB7Ku3fvMBqN8Otf/3ppfXihEUyxPV/qUeu2XC6j2+3KHOCWX5Meoc6JbpmlwMNQ9TZr4C7b4dWrV9jc3PzpSjvqzunfi14H7iNduo88Y54bIRzHQaPRkFQyzWGZgRVbHms4HJZSa6brYaIcG82wrJgGXSNbDlTSKuSvdTF2XTqPuY4auZN/4wYKjeq0S6lPPNWZEF5Q6zxvYFXUa15Do2vjck3DorekarTL1/TOu9FohNvbW0SjUXS7XUEei9qt0bUO3vF0XAZf+v2+BDoTiYTUYH2IK22KGaDWY8gcS+bioBcOvTDpgts67clNJ/MAiempaVSuc+VZXGdVWcbwmp4KP69Bny5mM51OpUodAY95vc6o0qUIuKBpG0Tgx4wQ0+AuY1uWqjJmulu2CaVdPioiFoshlUqh2+0inU6j2WwKNVAsFlEoFDCdTsWg6EMJ+YDZWZ/PJ8VvBoMB0uk0nj59KpPjpxZzYOhkaW0wiJrYTiJVTnjuVEulUtje3p5BvIzMsqoavQYAUpuhXq/jT3/6E7766iv88pe/xM7OjiTEs/oW3SnTXdf90JPPRA2r6kePAb0/Xw9wIrhWqyXegNYhP6PHF+/TbDYRDofxn//5n9jZ2cFf/uVfyhFPup/0Ooik+Rz4OV33g6CAvGmr1UK5XMZvf/vbmToXy+pCizlvgFkDp6kT/tAQcE7wZIdOp4PxeCy56X/+85/R7Xbx61//Wkou6oCt2RYNRHSwkACJ45ULFflMZi84joNKpSJb972KW1sWxRz0IqyBGFE4x91kMpFFUx+9bgvWcrGnx0Uw02q1ZDzQS43H49jf35fKfm7ihdr0VGVMK8NmhE3l6S8l18KVWEfa2Rm6x3qymfvJyV0CHyofcTDwHCuzLfPQ3DwU4FX0hDE5SI1e9AOnW+33+6UsHtORWHNBGxnTfdY6+uGHHxAOh6WuLnC3OFE/8xL63Xi7x+Qugfu1FmjszJKOJtJlH7QbTk+AtTrOz88BfNhEokXzv9pF19t5OYa4EDK412635Wyx0Wj0oI0RbIvWrQ7+2Thc7QkQ7WpqgTqbTqfStmaziWg0KtW/dIU2r23UXhrHKfVHb5KHmnIH10OQrtaN22sa4WpvAJjN/NA65pjRi4nWM4VzU1c3pH71uCQVyG3Ei8bDorHiKWXMTaiIeSsXFcA0jXQ6jeFwiGKxCODO4FQqFbx79w6O86EeKA3J9va2HKwYDofhOI5A//F4jHg8LvybmyExDe9D6AVOfAY1iJxIf3AyRKNRJBIJALMncFAXekKdnJzIAhYIBCQTgztnaAwqlQpqtRrev3+Pm5sb2b5I9EO0yOyGw8NDcYs5ME03TXsvAFaO0Ov7a/Sh6wiwv5ysPGFWb2XlZ/h89e5HUi88IPH9+/doNBp4/fo1isWi8P7aWOhiOgCkLXx23FfPnY/k28vlMorF4gzvuqzhNY2DRmQmjaIDizo7gXv++Yx1lgeBC/DhAMZ//dd/xdbWFn73u99JQW5zXmjDT4NTr9elYDuD1+RvT05O5LDHQqGAQqGAeDyOTz/9FO12e6XxQv14pRZMg6v7QqHB5Unj9IyY4aADt8D9XaM8GFWj3OFwiM3NTTldQu/Oc6NVF/XH8440r2Ijujng6PKSbiBP1el0UKvVAEAGFM/74hE0euXWXIs+C4zfaRpVEwG7fc6L8EFrNEaeS6+8OnCmhQYkEAhI8R+99TkYDErJQu7q48Nn2pDmxIEPg4cDqtlsolqtYnNzE5PJZGayUwdm33XGyKqLEvXCv228pTYseqeP5g818qVuyStqZHN9fY3RaIT3799jPB6LkWQVKG7Ioehjs2l0dUBPPzdmArilpHkRTkyNwHw+3wzyMtG9GUzjsyU9olEunxtlMBjg7du3qNfr+Pzzz+9tIDGfFbOCuLni/Pxc+sk5ymwZFvLmluJwOIzt7e2VjzCyGS0vnK6pV1NMCofPm32l7rjg6EVOL/YEU0S6m5ubkmrmNh68jpGFSFdPHP26+T7/N/ld4MNRGPF4HJlMBtPpFFtbW+h0Ori+vsZ4PJbjkMvlMnw+34zRDYfDM9XJaJwikQgymczMUTXz+Cuz/asIJwgfiubEaDDIA3GbIY0hH7iOkpP3ZUrcdHpXZ6LT6aBer4tO+/2+ZICQlkilUshkMgCAXq+Hq6sr4XuJIs3nYw5Y8sVXV1eyP31VXldzxyy9qKkFjeaIMLjwmtW0aID0okpvRpdznE6n+OKLL/D27Vs8efJEAmscc/o4J/KVPIONKDiVSklcoFwuy2TVVJjbJJ8nbKfmg21GljrRQTMuBjygVacwUb9+/13NYZ/PJ7nrg8FAUjK73a7UXtaBWcYFhsO7I9d5GOhwOBSvst/v4/3795JBVCqV8Pr1a9kC7DiO1N19bFmkZ84T85lonep26QWGtsbv94vnQDBj5osDdzGJQqGA3d1d8Rw0eDFpEC/iiV5wS67WnXbjeXk9BzAfKI/FqFQqMtl8Pp8YWdb+5OBigIMDM5FIIBaLzewsYTvmKUAraRXjq3laGl0TydEtZmUnnSbFLA29i0ZfB0A2ATAXl/zZxcWFuIA+n0+2MjrOXWYDC36Q/9PtM58X+wLcUQrHx8eIRqMzJ3EsqxcaVk0rmMaGC6etapM2Qtoj0bnYtkDUjz/+iEDg7jijRCKBzc1NWbRZUtPv94vrWK/XcXNzI5OQxnZjY0MW/VUmkymmK29yuLq/+ocGlqlMehGiR8Axz0UhELg7H4z8NSmrnZ2dmcwYjkOWLLy8vMT19bXkevt8d1k1nU4H1WpVxnEul0OxWJypDUIj9FOLDfDZxKRtzGs4PlmnheOB9IL2JDgXWXKVxXFs3+nWVjfxdBrwvB/95XriafTn8/lmaiJwUjK9I5FIIJlMyoGAfE27gvzNgfb69WuppTuv+IRpXB9KL3A3jt7AYN5b953IRbvtNLSmLrUr5DiO1JMAIGlN0+ldzQnen/VVtTu6tbWFYrGI7e1tieRSf5x45LAuLy9RrVbx7bffIp1OY39/fyW96EGqFxV+r47Om/SBNjh080KhkCysjJrr4u5a36QeiIg5bun6MjBUq9WkWlS328X29jaePn2KUqmEra2tGfrBlFV0wgVIe2F68dEUFXViLky6shUwi541+ucPj5x59+4dqtWqBIC4uNVqNTlWnEXQR6OR1BKgh0V+c2dnBx999NE9b5L9W1ZMDneVBd70pk096sCY4zgC3PSuM8dxZvh8syg8dZJOp7G9vY1UKjVzXNNDvOWVUsbcXC0zEmtSDKZhZJGXWCwmx23Q6DJ9igVyeG8S/EdHRzg6OpLq9TbDyu9x+38VxdG113yc22rH93X7TcpGDz4aY6ISbq5gnWEOFq64zIYgJ+zz3XGqmUwGz58/l22R+lkQVXc6Hdzc3OCrr75Co9HA6ekp8vm8ZFYsKzQwrNqkA2ja2GsenEZX73PXGSsMvpKqoNHVxxRRuIiTB6eHQT22223c3NygXq/Ls0mn03j9+rXwt2wP3+dzsXHgXnVibjbRRkJztzrDRS9A1I0OypFWoAfJseD3++XImouLC1QqFaTTaeTzefnO09NTScckxUWPMhgMotVqoVaryfcmEgns7e1Zq989lFqwGeBl5qRpb2ybX3hfTS9wIWMqYafTQbfbFe6c45C0ClPw3Dz+ZWWu0TWji26uquli2waoGXjSObx84FoxTEXRSeyj0Qj7+/vY2Ni4R2yzHWzzIsPL/i0rOjDGSeo4jqyCzItl1LlWq6HRaMyQ81yBdTSeOtRHsehDBzVPtbW1hUQigaOjIzmOJRwOI5/PI5lM4uDgQHZQ8Ww5BuS4A+z9+/cYDodIpVKIx+OS48u8xVWExkB7NeaE0NytmS6m78NNL/F4XHJ9aXTNesPAh2c8Ht8VdydPrlP29LihnlhL1Zbyx79Xlf/6r/9CKBSSqDe/ixPbTJXTnLc+hoc/Gu3qDRa6/xxHnBfcAUqEzMwOepPsH/ndq6srXF5eYnNzEy9fvsT29vY9jl9/17LyEINlLoQmJUMunIhVgzTSL5ybOqZgUgsED5ubm3jz5g1yudzKvL5NFhpd0w1mp7Xo92zIT7sBOnrIwt2cqLo2Ae9BY8vo/O7urhwjbXIs84ypjU5YhWIwsxE02qKx4etEVzc3NzPpKa1WS3SWSCRQKBQE9TMQwsmhaQzSNuVyGYVCAZ9++il2dnYkh5IlNWmkiFposBiYubq6wnfffYdYLCZH/ZRKJQCQAbms0LAT6VIPJnqjobHtbdf3Ib3AylDsgy6qZPNaqHcActAn0w3L5TK2trawubmJQqFw7+gg4H6U/CFG95//+Z8RiUTw0UcfoVAo4OXLl1KKkQvPZDKRfpl8rvaQtAdAr8iWR0+908s4OzuTgCSDz8yQicfjsvgNh0Phuo+Pj1EsFnF0dCQHwz4WaLGBOC/GzHzOOpht5lsz8MhrSMXwc+ZmGXPjDMdfuVzG3t6enAztpV9exBO9YHvN5iJrBdlSX2iwSM77/XfbOjV0124oOcvRaCSre6lUQqFQmDkuxFx13agPWzuXXbl02pptQeLDu76+xsXFhXCmjJKST6SRBSB1Tx3HmXF3OAFZTezVq1fI5XL47LPP5DA9ut78fgZfuKKzgPpkMpEzn/x+Pw4PDxGLxVAqleA4juwMu7i4wGQywR/+8Iel9JJIJGYCXNoV1WiN48FEujR85OyZJsgJow2M5qjJm3OsAJBFnPmkPICQ/zPIZhqTRbKskSGi/fOf/4yzszO0Wi1JB9Tzh8+PRoFHUDHLghF1jhGNwvVioekuHTvRHogupq8XQ6bZ9ft9ZLNZFAoF5PN5iQnYPMifWrSBtdkZHTMCIPohvcQFhkFHnWHEgBoRsh6HhUIBuVwOOzs7shniMcUzp+v2mg39auivk+J1YIHE/Hg8ntmJZe4eabfbGI/HskpvbGwISrEZTLM9tkDXQ1wETlS2U0fUAYihOz8/x+npKS4vL3F1dYVqtYp6vS4DmBX+fT6fnHFGA8uTMDggWGPg1atX2N3dxR/+8Afk83lBidSx3r6okTINPJHMxsYGfvGLXwiX1263cXp6ilqthm+++Wal3Etu0SYiMF17zedqdGX2gQaC6NTtPDTtOdEgsUBQJpNBLBbDy5cvsb+/j+3tbSl4out7AN55yVWMDY/8rlQqcBwH3333HcLhsHgnNP4U9rHdbsuhk8yR7ff7M0FEGiJTTE+T1AD1qfWon0etVhMOOJfLiTfFzwEPz/zRoheOeZ6zNvg6Y8WkMvm3pi6JWAnq9BjUOdBmjm6pVMLLly9RLpcFTJjykP4vdQS72w87rY2PHtQ6CMBBRoOg+RmfzycV2rWxBoDt7W0UCgWp9m4uBG6T4iH80zy9cPXTdW6n0ylqtZpUKdKn2/J9DgQOBib+66irRoXxeByvX79GLpfDX/zFX6BcLgsaYhodUbcOTgGQlDUaJyZ553I5OaCvUqng9vYWp6enaLVaMzt2VhETebE9eoLTiNjq59ooJ43kGTycTqdSG4AcOdMNnz59ilwuh+fPn0vVLXOTgxvv/5hojv3iQs2+VqtVtNttKVBjeo+6xjANN40s9WuWMuX32QKBnE/a5dYB4el0inQ6jVQqhVKphO3tbezt7c3oxuyX7fVlxI2qnPd5MxDPRYjgDZjdfMRFm9fpxZ4AR6flkcIql8szBvcxbQew5OYI88c2kPUEMQMAnFAM6ujoLo2NJvY5UUOhEF6/fo2dnR1ks1lr8r4bpcD3Hsvw8j5MSucDZUWxk5MTCUYwaq55p1gsJrVPaSSI9BhB1TxnOp3Gb3/7W+zv7+Pjjz9GJBLB+fk5qtUqvvvuuxn0zEAa61kw8s8MiEQiIdtl0+m0INtKpYJvv/32Hl+9im446fkszSAHOTf+cAswnxUpBZa95AKljTdPhtb5zMFgEDs7OygUCvibv/kblEolCcKZCwFw38gu4ilXGTt63NPoktfXYuYfk4rSQVrOGwabOfbMuaBT9/T41Fw6wQy9zHA4jL29Pezt7eHp06fY3d0VT9IWB1nWYLqJjRJ0A1PaS9EUAvWsdUJDy8wmAg9d7ZA7PKnr4XCIQqEgaZPcVKK53MdajB9WUgqz7gEHhem+mS6+yfMR4VI5VHYgEMCTJ0+QTqexubk5Q2ibrqZN5iEb8x7L9Fdfz0VFGxPt2us+6jqqAGb4WhqR6XQqFcZevXqFQqGAw8ND5PP5mcHCYBzL13Fy6nQtFupgW2isiDSr1SoqlQpardYMIlhFtJsHYEYvDFawmAyNLdtl6tSsM8vPaIRCPcViMezv7yMWi+Ho6AiFQkFOjDb7YxsnPyVPabq+7I/mvclFk0Lj9nCmO3EeDQaDmU0O2ggRAWtvh9ezDRwb5Mvz+TxCoRAymYxUuiP3rTcb2eQhIMakRUyaYd4CaFIKbAv1YhpdAkDOQSJaXb5RU1zZbFbsDe+h2+NlrHjRyVJHsJs3ZoM08mUUVk8WPSB0YI2Di+4v6Qe/3y8u5m9+8xtsbm5Kor+50toevs1t9KqQRcIBz3sx2EVKgUc2E7Gyv0ze19XSaIhqtZpMFPLW5XIZv//971Eul/H8+XP4/X4cHx+j1WqhWq1iOBzKLj8aOEb7o9HoTP0AFn5htSoawWaziZOTEwCQ4jyr5l7qIA8NpI4U87QLFu1hWpeuvcBnqQMfpJ3I3Q4GA1xdXSEYDGJvbw+5XA6/+93vUCgUUCwWZ7bt6sV/nsF1Q736M6sYZtPV1/NBc4yXl5dyJBOzM5jRoI0Ghd4RETTvrTl0/lC/5I93dnaQy+VweHiIzc1NbG5uym4rffqI1p1pBDkHVxkrOu1N39NNv7otJperMzUINABIkJ79YUaOHotMn9QU187ODt68eYONjQ3X+smPYUOWKnhjGjz+cBXlANGpLzSwdC/NKj7aDdWV3jmhisUiMpnMDIe1qPM2vo7yGKiG7QUg6MusAqWjoZpn0tWydE4g60ik02kcHBygUCjg4OBATgKme9RsNnF5eYnBYCCTheiZbeKAoeFl4R0abLqX5EQ1IlzV6PI6Tnp9+gWfu64FqyvFkVoyKSud1UAaYTKZIJ1OI5FISOog3cJFCM18hl5Qy0NoKdOg6AVAGzTNSw4GA8k2oS6CwaBs/QYgwVFtdDk3zB1qHHPM3Njf35fDK3m0j3moqa3/j+UNsPi+rkKnf5v5sMy+0B60Hhf8MQ2yNtRmSpmZscD7s6wA7Zhp60zdzJN573s6gp1i7sigksghkjPRP3S7WZy8VqvJ1jtOfj5QRtkTiQT+7u/+DqVSCaVSaWYgLdM5ihvyXVXIQTO7otvt4urqSpAuX9Pkvh7YepNAIpFAJpORZPSDgwP89V//tbiBjuMIwmMa2ldffYVOp4MXL14gn89LYR1ynKxxwXvX63UMh0PhbjkoNS+oEeqqqI5t1ZWpiC6ur68lb5lFVsg1s1i93oarxwYpkevra8TjcRweHqJcLuPv//7vkclkJG1OG9JV+jBvoV5l3JgIl4iKqJWcNb2M6+tr3N7eyiKsDScNA6/RKJr1AWKxGAqFglQDC4fDKJVKSKVSePXqFTKZDHK53EwWgzZY8wKoj2V4v/zyS1lIuNgQ3WtKgJLNZiXLh140wZr2knQcRBtbzaUzFZNHteuKhqw7wg0jrAWjf6iHeb8p84pGea6na7u5LtpiEvtcTZjGZCZ9U1H6QQYCAckR5E4r0/Db2rKIq7NNILfrFgkHpi6jyF1e5Cp1qpx2hXQRd/5OpVIoFovI5XJSB4DGmqlQHCy6VKHP55NNAHo/Po11KBSSrb7X19eyK44rOZEVERS5NhaIWUUvHOB6l49egGlgOQZsmRJ0iRkwooRCIWxvbwvCLRaLMyjN5t0s+3zdPKKHcphmGqT+m0hUp0DSAHJO8bvp8ejUOhb3YYU1Hr/OmszcVs+MFbM41CI9mfPooYaXOyTpuTLOwXluliFl0SLTS+DY5d/0ljj36IHrZ8BrdIBfG2nmReu9Ayb61gFPrUO9QAIPMLoUN4uv4bjjOPJAub2uWq3i+PhYVhm6l+PxWFJhKI5zl7v65s0bHBwcYGtrSyK2i+C9bTA8xC2cJ0S4jUYD/X4fZ2dn6HQ6uLi4kMIZusoXfxKJhOz8KpVKyOVy2N7eRqlUwv7+vgS9WMKx3W7j/PxcdKfznrn9+ezsDMPhEL/4xS9QKpXw9OlTyU4Ih8P48ssv8fXXX0uALxaL4fDwUGiHRqOBH3/8UdBDJBJBqVRaKRmcWQjkyW5vb+WZs6oVi6mwXOFgMLg3qElJAJBgWSqVQjabxUcffYTNzU188sknchSRdjs1gge8Gwlt7NyuWWUsaerMBjK4wBDhc7EEIAuPTurXUq1Wsbe3h1/+8pfY29vD3/7t38rGDzOwpNuu9cQ+m/3zaoQXfdYm79+/B4B7xtXUL3UXCARk7rD9ur40uf7xeIxmsznjNfP+mtrkpiNzC7HP58PXX389U3GNgFLTPBrYmAidgclAIDAXvKyUvWC6PeZ7dJm020cEyFXcxqkFAgFsbGwgl8vN7ELyMuDnIZ3H5KTo2uiycHSTNVdEV5CbQIjcNzY2sLW1hUwmI7waUQmDYsxOYCpLo9GA4zhyL/7mqtxutxEMBrG9vQ2fzyeTnEfec2Ax8Z1HrnBQ85kRea9ST5eGzyxLyAWYemIAlUhDB1Q0EiTCicfjyOfzyOVy2N3dRbFYFMSmXWT93HWb3Npqk8fmMDUHaQandBYCv1sjT1sAiUI9O44jBW3I75vzhvrQHLLX/tt+83M6X3oZsX3e1h7dbxvfbKJXvWWeuqXwM2aQ0dSrDq5pg8vAJhdHHdjkok/9eNHH0rOLFp/5k7qSFN/PZrOSenJ7eyu1XlnEhq6jNop0l46OjlAqlazJ317aBthX78eaSJVKBaPRCJVKBb1eD5VKZSYViggun88jkUjgxYsXODg4wM7OjhjZbDYr6JKDh6sjo/PcP85aDIFAAAcHB0ilUjOHKA6HQ7x9+xaTyQTZbBbFYhGXl5doNpv48ccfUalUhDtLp9OS68uSf99//z1CoRD29vbktNNVjC77QUN7c3MjB/31+33RExEvk9c1/zYYDGSgF4tFvHjxApubm/j4448Rj8eRy+VmotTaiJnP2Csa42RZZKxXQbo0jHpbO40Cg2H8X7usuh2mYWAfublhZ2dnZruqThWbpwNbf0zv0M3w+v132/cvLi4wHA6xu7vrWScM8ppz1c1TZXlPHTQkcOMCTuqMnpPe4k0KkADGNNAEHqFQSOhQto0IlgclcH5pioe0EO2iF3kQ0nUziDqVgw0x8zG16IGvMyGWMbg/l2i3RLv8Ok2O/ChrTLAuZyqVkr335G15HfBhIJquqM7z5flgGiVx8PFeZpUq3peGTm86YDt1qtmqSFejM41EbPoyswx4DSc1dZdMJpFKpe7RCY/lubjJYyzSptGch4K87nwimuK9TDfYrR3z7rdqP2nAlhGNlk0xFxeOTW0HzIVEzzlbP0wPyraQ6c/a3tOUBfVMBL7IFlp18FMP3rWsZS1rWcsHWX0L0lrWspa1rGVpWRvdtaxlLWv5GWVtdNeylrWs5WeUtdFdy1rWspafUdZGdy1rWctafkZZG921tpd0EgAAAA1JREFUrGUta/kZ5f8BuD7rBmzgylwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8AVb_qUFJIG"
      },
      "source": [
        "Normalizing the data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpn4WesUFUxg"
      },
      "source": [
        "def img_norm(data):\n",
        "  output = []\n",
        "  for img in data: \n",
        "    img = (img - np.mean(img))/np.std(img)\n",
        "    output.append(np.array(img))\n",
        "  return np.array(output)\n",
        "\n",
        "x_train_norm = img_norm(x_train)\n",
        "x_val_norm = img_norm(x_val)\n",
        "x_test_norm = img_norm(x_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62rDEbZpSKl6"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_train[:,None], x_train_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_val[:,None], x_val_norm),axis=1),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    np.concatenate((t_test[:,None], x_test_norm),axis=1),\n",
        "    batch_size=1000, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTzKhbQxR3ik"
      },
      "source": [
        "Defining CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6cZcnLpR1yD"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, n_feature, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.n_feature = n_feature\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
        "        self.fc2 = nn.Linear(50, output_size)\n",
        "        \n",
        "    def forward(self, x, verbose=False):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = x.view(-1, self.n_feature*4*4)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "def get_accuracy(model, data, batch_size=50):\n",
        "    #Compute the model accuracy on the data set. \n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        xs = torch.Tensor(data[i:i+batch_size])\n",
        "        zs = model(xs)\n",
        "        pred = zs.max(1, keepdim=True)[1]   # get the index of the max logit\n",
        "        pred = pred.detach().numpy()\n",
        "        correct += (pred == 1).sum()\n",
        "\n",
        "    return correct / data.shape[0]\n",
        "\n",
        "def valid(model, perm=torch.arange(0, 784).long()):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    correct = 0\n",
        "    for ar in val_loader:\n",
        "        data = ar[:,1:]\n",
        "        label = ar[:,0]\n",
        "        # send to device\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        \n",
        "        # permute pixels\n",
        "        data = data.view(-1, 28*28)\n",
        "        data = data[:, perm]\n",
        "        data = data.view(-1, 1, 28, 28)\n",
        "        output = model(data)\n",
        "        valid_loss += F.nll_loss(output, label.long(), reduction='sum').item() # sum up batch loss                                                               \n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "        correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    valid_loss /= len(val_loader.dataset)\n",
        "    accuracy = 100. * correct / len(val_loader.dataset)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print('\\n Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        valid_loss, correct, len(val_loader.dataset),\n",
        "        accuracy)) \n",
        "\n",
        "def train(epoch, model, lr=0.01, momentum=0.5, perm=torch.arange(0, 784).long(),max_iters=1000):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    n = 0 # the number of iterations\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "    for batch_idx, ar in enumerate(train_loader):\n",
        "        data = ar[:,1:]\n",
        "        label = ar[:,0]\n",
        "        # send to device\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        \n",
        "        # permute pixels\n",
        "        data = data.view(-1, 28*28)\n",
        "        data = data[:, perm]\n",
        "        data = data.view(-1, 1, 28, 28)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, label.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item())) \n",
        "        if batch_idx % 400 == 0:\n",
        "            valid(model, perm=torch.arange(0, 784).long())\n",
        "            \n",
        "        if batch_idx % 100 == 0:\n",
        "            iters_sub.append(n)\n",
        "            train_cost = float(loss.detach().numpy())\n",
        "            train_acc = get_accuracy(model, train_loader)\n",
        "            train_accs.append(train_acc)\n",
        "            val_acc = get_accuracy(model, val_loader)\n",
        "            val_accs.append(val_acc)\n",
        "            print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                  n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "        # increment the iteration number\n",
        "        n += 1\n",
        "\n",
        "        if n > max_iters:\n",
        "            return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "accuracy_list = []\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, perm=torch.arange(0, 784).long()):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for ar in test_loader:\n",
        "        data = ar[:,1:]\n",
        "        label = ar[:,0]\n",
        "        # send to device\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        \n",
        "        # permute pixels\n",
        "        data = data.view(-1, 28*28)\n",
        "        data = data[:, perm]\n",
        "        data = data.view(-1, 1, 28, 28)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, label.long(), reduction='sum').item() # sum up batch loss                                                               \n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "        correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        accuracy))   "
      ],
      "metadata": {
        "id": "CL14te9WHoVG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "-8PZXeXgSgDh",
        "outputId": "89b5e863-9a33-492d-e34e-5becbfbc0a8d"
      },
      "source": [
        "# Training settings \n",
        "n_features = 10 # number of feature maps\n",
        "\n",
        "input_size  = 28*28   # images are 28x28 pixels\n",
        "output_size = 26      # there are 26 classes\n",
        "\n",
        "model_cnn = CNN(input_size, n_features, output_size)\n",
        "model_cnn.to(device)\n",
        "\n",
        "lr = 0.001\n",
        "momentum = 0.1\n",
        "\n",
        "for epoch in range(0, 6):\n",
        "    train(epoch, model_cnn.double(), lr)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/21964 (0%)]\tLoss: 3.249300\n",
            "\n",
            " Validation set: Average loss: 3.2588, Accuracy: 335/5491 (6%)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1821bb25f8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-b6d2eefcee37>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, lr, momentum, perm, max_iters)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0miters_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b6d2eefcee37>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, data, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# get the index of the max logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model_cnn)"
      ],
      "metadata": {
        "id": "nWSxITJGJ4VW",
        "outputId": "8bf087bb-732b-44eb-a9ab-f9f2ac864e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0496, Accuracy: 5817/7172 (81%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use CapsNet:"
      ],
      "metadata": {
        "id": "mWnFPOaVIavV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from capsnet import CapsNet\n",
        "from data_loader import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "USE_CUDA = True if torch.cuda.is_available() else False\n",
        "BATCH_SIZE = 100\n",
        "N_EPOCHS = 30\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "n_features = 10\n",
        "'''\n",
        "Config class to determine the parameters for capsule net\n",
        "'''\n",
        "\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # CNN (cnn)\n",
        "        self.cnn_in_channels = 1\n",
        "        self.cnn_out_channels = 256\n",
        "        self.cnn_kernel_size = 9\n",
        "\n",
        "        # Primary Capsule (pc)\n",
        "        self.pc_num_capsules = 8\n",
        "        self.pc_in_channels = 256\n",
        "        self.pc_out_channels = 32\n",
        "        self.pc_kernel_size = 9\n",
        "        self.pc_num_routes = 32 * 6 * 6\n",
        "\n",
        "        # Digit Capsule (dc)\n",
        "        self.dc_num_capsules = 10\n",
        "        self.dc_num_routes = 32 * 6 * 6\n",
        "        self.dc_in_channels = 8\n",
        "        self.dc_out_channels = 16\n",
        "\n",
        "        # Decoder\n",
        "        self.input_width = 28\n",
        "        self.input_height = 28\n",
        "\n",
        "def new_margin_loss(x, labels, size_average=True):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        v_c = torch.sqrt((x ** 2).sum(dim=2, keepdim=True))\n",
        "\n",
        "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
        "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
        "        print(v_c)\n",
        "        print(batch_size)\n",
        "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
        "        loss = loss.sum(dim=1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "def train_capsnet(model, optimizer, train_loader, epoch):\n",
        "    capsule_net = model\n",
        "    capsule_net.train()\n",
        "    n_batch = len(list(enumerate(train_loader)))\n",
        "    total_loss = 0\n",
        "    for batch_id, ar in enumerate(train_loader):\n",
        "        data = ar[:,1:].type(torch.double)\n",
        "        target = ar[:,0].type(torch.int64)\n",
        "        target = torch.sparse.torch.eye(26).index_select(dim=0, index=target)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        perm=torch.arange(0, 784).long()\n",
        "        data = data.view(-1, 28*28)\n",
        "        data = data[:, perm]\n",
        "        data = data.view(-1, 1, 28, 28)\n",
        "\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, reconstructions, masked = capsule_net(data)\n",
        "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        correct = sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))\n",
        "        train_loss = loss.item()\n",
        "        total_loss += train_loss\n",
        "        if batch_id % 100 == 0:\n",
        "            tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
        "                epoch,\n",
        "                N_EPOCHS,\n",
        "                batch_id + 1,\n",
        "                n_batch,\n",
        "                correct / float(BATCH_SIZE),\n",
        "                train_loss / float(BATCH_SIZE)\n",
        "                ))\n",
        "    tqdm.write('Epoch: [{}/{}], train loss: {:.6f}'.format(epoch,N_EPOCHS,total_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test_capsnet(capsule_net, test_loader, epoch):\n",
        "    capsule_net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for batch_id, (data, target) in enumerate(test_loader):\n",
        "\n",
        "        target = torch.sparse.torch.eye(26).index_select(dim=0, index=target)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output, reconstructions, masked = capsule_net(data)\n",
        "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        correct += sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
        "                       np.argmax(target.data.cpu().numpy(), 1))\n",
        "\n",
        "    tqdm.write(\n",
        "        \"Epoch: [{}/{}], test accuracy: {:.6f}, loss: {:.6f}\".format(epoch, N_EPOCHS, correct / len(test_loader.dataset),\n",
        "                                                                  test_loss / len(test_loader)))\n",
        "    \n",
        "\n",
        "torch.manual_seed(1)\n",
        "config = Config()\n",
        "\n",
        "capsule_net = CapsNet(config)\n",
        "capsule_net = torch.nn.DataParallel(capsule_net)\n",
        "if USE_CUDA:\n",
        "    capsule_net = capsule_net.cuda()\n",
        "capsule_net = capsule_net.module.double()\n",
        "\n",
        "optimizer = torch.optim.Adam(capsule_net.parameters())\n",
        "capsule_net.margin_loss = new_margin_loss\n",
        "for e in range(1, N_EPOCHS + 1):\n",
        "    train_capsnet(capsule_net, optimizer, train_loader, e)\n",
        "    test_capsnet(capsule_net, test_loader, e)\n",
        "    # train(e, capsule_net)\n",
        "    # test(capsule_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SL73PRG8IZlv",
        "outputId": "90f17792-b9c8-4b04-d460-a228fef7c000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[7.2040e-04]],\n",
            "\n",
            "         [[3.9812e-04]],\n",
            "\n",
            "         [[1.0133e-03]],\n",
            "\n",
            "         [[5.3254e-04]],\n",
            "\n",
            "         [[5.1853e-04]],\n",
            "\n",
            "         [[2.8751e-04]],\n",
            "\n",
            "         [[1.0886e-03]],\n",
            "\n",
            "         [[4.4011e-04]],\n",
            "\n",
            "         [[4.5577e-04]],\n",
            "\n",
            "         [[1.5533e-03]]],\n",
            "\n",
            "\n",
            "        [[[4.8048e-04]],\n",
            "\n",
            "         [[9.5475e-04]],\n",
            "\n",
            "         [[1.0348e-03]],\n",
            "\n",
            "         [[7.9844e-04]],\n",
            "\n",
            "         [[9.5175e-04]],\n",
            "\n",
            "         [[4.6793e-04]],\n",
            "\n",
            "         [[8.2707e-04]],\n",
            "\n",
            "         [[8.2790e-04]],\n",
            "\n",
            "         [[6.2240e-04]],\n",
            "\n",
            "         [[5.8274e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.3844e-04]],\n",
            "\n",
            "         [[3.0436e-04]],\n",
            "\n",
            "         [[7.0378e-04]],\n",
            "\n",
            "         [[2.3041e-04]],\n",
            "\n",
            "         [[3.9539e-04]],\n",
            "\n",
            "         [[4.3348e-04]],\n",
            "\n",
            "         [[6.2572e-04]],\n",
            "\n",
            "         [[5.6007e-04]],\n",
            "\n",
            "         [[7.1674e-04]],\n",
            "\n",
            "         [[4.6842e-04]]],\n",
            "\n",
            "\n",
            "        [[[7.9816e-04]],\n",
            "\n",
            "         [[8.2185e-04]],\n",
            "\n",
            "         [[8.8241e-04]],\n",
            "\n",
            "         [[5.1925e-04]],\n",
            "\n",
            "         [[7.2608e-04]],\n",
            "\n",
            "         [[4.0980e-04]],\n",
            "\n",
            "         [[1.4763e-03]],\n",
            "\n",
            "         [[1.0250e-03]],\n",
            "\n",
            "         [[9.9145e-04]],\n",
            "\n",
            "         [[1.9360e-03]]],\n",
            "\n",
            "\n",
            "        [[[7.2517e-04]],\n",
            "\n",
            "         [[6.1359e-04]],\n",
            "\n",
            "         [[9.2852e-04]],\n",
            "\n",
            "         [[8.6814e-04]],\n",
            "\n",
            "         [[4.2010e-04]],\n",
            "\n",
            "         [[2.1940e-04]],\n",
            "\n",
            "         [[7.3615e-04]],\n",
            "\n",
            "         [[4.2255e-04]],\n",
            "\n",
            "         [[8.9713e-04]],\n",
            "\n",
            "         [[5.9905e-04]]],\n",
            "\n",
            "\n",
            "        [[[7.8648e-04]],\n",
            "\n",
            "         [[7.4784e-04]],\n",
            "\n",
            "         [[6.8559e-04]],\n",
            "\n",
            "         [[5.1560e-04]],\n",
            "\n",
            "         [[2.4142e-04]],\n",
            "\n",
            "         [[4.0675e-04]],\n",
            "\n",
            "         [[9.7997e-04]],\n",
            "\n",
            "         [[6.0482e-04]],\n",
            "\n",
            "         [[1.1800e-03]],\n",
            "\n",
            "         [[5.8926e-04]]],\n",
            "\n",
            "\n",
            "        [[[9.5078e-04]],\n",
            "\n",
            "         [[6.1261e-04]],\n",
            "\n",
            "         [[1.6204e-03]],\n",
            "\n",
            "         [[4.1452e-04]],\n",
            "\n",
            "         [[2.8784e-04]],\n",
            "\n",
            "         [[3.1569e-04]],\n",
            "\n",
            "         [[9.8299e-04]],\n",
            "\n",
            "         [[6.3354e-04]],\n",
            "\n",
            "         [[8.7776e-04]],\n",
            "\n",
            "         [[6.5053e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.0138e-04]],\n",
            "\n",
            "         [[9.0057e-04]],\n",
            "\n",
            "         [[1.2455e-03]],\n",
            "\n",
            "         [[1.0070e-03]],\n",
            "\n",
            "         [[3.3103e-04]],\n",
            "\n",
            "         [[3.3141e-04]],\n",
            "\n",
            "         [[1.2351e-03]],\n",
            "\n",
            "         [[6.0606e-04]],\n",
            "\n",
            "         [[9.5251e-04]],\n",
            "\n",
            "         [[6.4511e-04]]],\n",
            "\n",
            "\n",
            "        [[[9.3905e-04]],\n",
            "\n",
            "         [[8.0321e-04]],\n",
            "\n",
            "         [[1.2365e-03]],\n",
            "\n",
            "         [[1.7995e-03]],\n",
            "\n",
            "         [[8.4828e-04]],\n",
            "\n",
            "         [[9.8818e-04]],\n",
            "\n",
            "         [[1.0750e-03]],\n",
            "\n",
            "         [[7.5724e-04]],\n",
            "\n",
            "         [[6.9079e-04]],\n",
            "\n",
            "         [[9.5196e-04]]],\n",
            "\n",
            "\n",
            "        [[[7.5622e-04]],\n",
            "\n",
            "         [[5.0224e-04]],\n",
            "\n",
            "         [[1.0318e-03]],\n",
            "\n",
            "         [[4.1514e-04]],\n",
            "\n",
            "         [[4.2105e-04]],\n",
            "\n",
            "         [[7.2301e-04]],\n",
            "\n",
            "         [[9.0516e-04]],\n",
            "\n",
            "         [[4.3624e-04]],\n",
            "\n",
            "         [[9.9148e-04]],\n",
            "\n",
            "         [[6.4020e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.9403e-04]],\n",
            "\n",
            "         [[4.7322e-04]],\n",
            "\n",
            "         [[5.6210e-04]],\n",
            "\n",
            "         [[4.8398e-04]],\n",
            "\n",
            "         [[4.2377e-04]],\n",
            "\n",
            "         [[5.9453e-04]],\n",
            "\n",
            "         [[6.6969e-04]],\n",
            "\n",
            "         [[4.9928e-04]],\n",
            "\n",
            "         [[6.7600e-04]],\n",
            "\n",
            "         [[3.9211e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.5335e-04]],\n",
            "\n",
            "         [[4.9430e-04]],\n",
            "\n",
            "         [[2.6708e-04]],\n",
            "\n",
            "         [[5.2486e-04]],\n",
            "\n",
            "         [[5.1218e-04]],\n",
            "\n",
            "         [[1.3809e-03]],\n",
            "\n",
            "         [[1.0955e-03]],\n",
            "\n",
            "         [[4.5338e-04]],\n",
            "\n",
            "         [[7.5852e-04]],\n",
            "\n",
            "         [[5.8416e-04]]],\n",
            "\n",
            "\n",
            "        [[[2.0049e-04]],\n",
            "\n",
            "         [[3.7296e-04]],\n",
            "\n",
            "         [[2.8317e-04]],\n",
            "\n",
            "         [[5.0369e-04]],\n",
            "\n",
            "         [[4.5135e-04]],\n",
            "\n",
            "         [[3.2720e-04]],\n",
            "\n",
            "         [[4.3819e-04]],\n",
            "\n",
            "         [[3.9013e-04]],\n",
            "\n",
            "         [[4.0032e-04]],\n",
            "\n",
            "         [[2.7866e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.0004e-04]],\n",
            "\n",
            "         [[6.3434e-04]],\n",
            "\n",
            "         [[3.0911e-04]],\n",
            "\n",
            "         [[5.9138e-04]],\n",
            "\n",
            "         [[2.9260e-04]],\n",
            "\n",
            "         [[2.8005e-04]],\n",
            "\n",
            "         [[5.6706e-04]],\n",
            "\n",
            "         [[3.9812e-04]],\n",
            "\n",
            "         [[3.3903e-04]],\n",
            "\n",
            "         [[7.0071e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.5142e-04]],\n",
            "\n",
            "         [[4.9567e-04]],\n",
            "\n",
            "         [[9.3273e-04]],\n",
            "\n",
            "         [[3.8770e-04]],\n",
            "\n",
            "         [[4.4121e-04]],\n",
            "\n",
            "         [[6.8729e-04]],\n",
            "\n",
            "         [[9.4957e-04]],\n",
            "\n",
            "         [[5.2271e-04]],\n",
            "\n",
            "         [[7.2065e-04]],\n",
            "\n",
            "         [[8.7515e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.2350e-04]],\n",
            "\n",
            "         [[4.1486e-04]],\n",
            "\n",
            "         [[9.7089e-04]],\n",
            "\n",
            "         [[5.9868e-04]],\n",
            "\n",
            "         [[3.7321e-04]],\n",
            "\n",
            "         [[4.0003e-04]],\n",
            "\n",
            "         [[7.1936e-04]],\n",
            "\n",
            "         [[7.2701e-04]],\n",
            "\n",
            "         [[1.4333e-03]],\n",
            "\n",
            "         [[6.1425e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.5571e-04]],\n",
            "\n",
            "         [[7.4932e-04]],\n",
            "\n",
            "         [[8.1339e-04]],\n",
            "\n",
            "         [[7.7109e-04]],\n",
            "\n",
            "         [[3.0884e-04]],\n",
            "\n",
            "         [[4.3186e-04]],\n",
            "\n",
            "         [[9.7912e-04]],\n",
            "\n",
            "         [[6.5344e-04]],\n",
            "\n",
            "         [[8.2855e-04]],\n",
            "\n",
            "         [[6.5319e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.0787e-04]],\n",
            "\n",
            "         [[5.1096e-04]],\n",
            "\n",
            "         [[5.0729e-04]],\n",
            "\n",
            "         [[3.6259e-04]],\n",
            "\n",
            "         [[3.1127e-04]],\n",
            "\n",
            "         [[3.8674e-04]],\n",
            "\n",
            "         [[4.0435e-04]],\n",
            "\n",
            "         [[3.1299e-04]],\n",
            "\n",
            "         [[2.9356e-04]],\n",
            "\n",
            "         [[6.5917e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.6439e-04]],\n",
            "\n",
            "         [[6.0337e-04]],\n",
            "\n",
            "         [[6.8966e-04]],\n",
            "\n",
            "         [[3.8079e-04]],\n",
            "\n",
            "         [[3.5799e-04]],\n",
            "\n",
            "         [[4.3313e-04]],\n",
            "\n",
            "         [[4.5876e-04]],\n",
            "\n",
            "         [[6.8749e-04]],\n",
            "\n",
            "         [[5.6501e-04]],\n",
            "\n",
            "         [[3.4592e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.8337e-04]],\n",
            "\n",
            "         [[1.0562e-03]],\n",
            "\n",
            "         [[5.8212e-04]],\n",
            "\n",
            "         [[7.8989e-04]],\n",
            "\n",
            "         [[8.1483e-04]],\n",
            "\n",
            "         [[7.5023e-04]],\n",
            "\n",
            "         [[9.9170e-04]],\n",
            "\n",
            "         [[8.5059e-04]],\n",
            "\n",
            "         [[7.7976e-04]],\n",
            "\n",
            "         [[5.2898e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.3749e-04]],\n",
            "\n",
            "         [[1.0526e-03]],\n",
            "\n",
            "         [[1.0386e-03]],\n",
            "\n",
            "         [[1.5978e-03]],\n",
            "\n",
            "         [[6.8669e-04]],\n",
            "\n",
            "         [[6.1303e-04]],\n",
            "\n",
            "         [[5.1705e-04]],\n",
            "\n",
            "         [[2.9067e-04]],\n",
            "\n",
            "         [[5.3528e-04]],\n",
            "\n",
            "         [[5.7136e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.3856e-04]],\n",
            "\n",
            "         [[1.0502e-03]],\n",
            "\n",
            "         [[1.0330e-03]],\n",
            "\n",
            "         [[1.5979e-03]],\n",
            "\n",
            "         [[6.8353e-04]],\n",
            "\n",
            "         [[6.1106e-04]],\n",
            "\n",
            "         [[5.1661e-04]],\n",
            "\n",
            "         [[2.9348e-04]],\n",
            "\n",
            "         [[5.3157e-04]],\n",
            "\n",
            "         [[5.7503e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.9325e-04]],\n",
            "\n",
            "         [[9.1156e-04]],\n",
            "\n",
            "         [[1.5506e-03]],\n",
            "\n",
            "         [[9.1033e-04]],\n",
            "\n",
            "         [[5.2273e-04]],\n",
            "\n",
            "         [[8.2423e-04]],\n",
            "\n",
            "         [[1.3555e-03]],\n",
            "\n",
            "         [[9.0040e-04]],\n",
            "\n",
            "         [[7.3769e-04]],\n",
            "\n",
            "         [[1.1769e-03]]],\n",
            "\n",
            "\n",
            "        [[[3.3662e-04]],\n",
            "\n",
            "         [[2.8499e-04]],\n",
            "\n",
            "         [[1.9884e-04]],\n",
            "\n",
            "         [[2.8832e-04]],\n",
            "\n",
            "         [[1.2792e-04]],\n",
            "\n",
            "         [[2.7658e-04]],\n",
            "\n",
            "         [[4.6708e-04]],\n",
            "\n",
            "         [[1.9339e-04]],\n",
            "\n",
            "         [[4.4960e-04]],\n",
            "\n",
            "         [[7.9844e-05]]],\n",
            "\n",
            "\n",
            "        [[[2.3704e-04]],\n",
            "\n",
            "         [[3.5688e-04]],\n",
            "\n",
            "         [[3.1696e-04]],\n",
            "\n",
            "         [[2.5189e-04]],\n",
            "\n",
            "         [[2.5012e-04]],\n",
            "\n",
            "         [[2.3551e-04]],\n",
            "\n",
            "         [[4.0787e-04]],\n",
            "\n",
            "         [[2.7773e-04]],\n",
            "\n",
            "         [[1.9163e-04]],\n",
            "\n",
            "         [[1.9208e-04]]],\n",
            "\n",
            "\n",
            "        [[[2.1674e-04]],\n",
            "\n",
            "         [[9.3491e-04]],\n",
            "\n",
            "         [[4.5377e-04]],\n",
            "\n",
            "         [[5.5207e-04]],\n",
            "\n",
            "         [[3.8639e-04]],\n",
            "\n",
            "         [[2.7629e-04]],\n",
            "\n",
            "         [[4.7047e-04]],\n",
            "\n",
            "         [[2.5804e-04]],\n",
            "\n",
            "         [[5.9911e-04]],\n",
            "\n",
            "         [[5.3716e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.8064e-04]],\n",
            "\n",
            "         [[6.0368e-04]],\n",
            "\n",
            "         [[1.0566e-03]],\n",
            "\n",
            "         [[6.6567e-04]],\n",
            "\n",
            "         [[3.3535e-04]],\n",
            "\n",
            "         [[5.6678e-04]],\n",
            "\n",
            "         [[8.3528e-04]],\n",
            "\n",
            "         [[5.6838e-04]],\n",
            "\n",
            "         [[1.1468e-03]],\n",
            "\n",
            "         [[5.1596e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.6513e-04]],\n",
            "\n",
            "         [[3.7713e-04]],\n",
            "\n",
            "         [[4.2353e-04]],\n",
            "\n",
            "         [[8.5372e-04]],\n",
            "\n",
            "         [[3.0038e-04]],\n",
            "\n",
            "         [[4.1563e-04]],\n",
            "\n",
            "         [[1.2216e-03]],\n",
            "\n",
            "         [[7.6901e-04]],\n",
            "\n",
            "         [[8.4710e-04]],\n",
            "\n",
            "         [[1.1047e-03]]],\n",
            "\n",
            "\n",
            "        [[[8.7154e-04]],\n",
            "\n",
            "         [[9.8905e-04]],\n",
            "\n",
            "         [[7.8905e-04]],\n",
            "\n",
            "         [[6.3223e-04]],\n",
            "\n",
            "         [[3.8728e-04]],\n",
            "\n",
            "         [[5.0990e-04]],\n",
            "\n",
            "         [[9.3801e-04]],\n",
            "\n",
            "         [[3.8086e-04]],\n",
            "\n",
            "         [[7.3519e-04]],\n",
            "\n",
            "         [[8.6817e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.4150e-04]],\n",
            "\n",
            "         [[5.2838e-04]],\n",
            "\n",
            "         [[4.3451e-04]],\n",
            "\n",
            "         [[9.2243e-04]],\n",
            "\n",
            "         [[9.1730e-04]],\n",
            "\n",
            "         [[4.2842e-04]],\n",
            "\n",
            "         [[8.9699e-04]],\n",
            "\n",
            "         [[7.5641e-04]],\n",
            "\n",
            "         [[6.8147e-04]],\n",
            "\n",
            "         [[6.7141e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.5499e-04]],\n",
            "\n",
            "         [[5.3178e-04]],\n",
            "\n",
            "         [[9.9810e-04]],\n",
            "\n",
            "         [[7.7513e-04]],\n",
            "\n",
            "         [[7.9460e-04]],\n",
            "\n",
            "         [[9.7890e-04]],\n",
            "\n",
            "         [[8.2531e-04]],\n",
            "\n",
            "         [[1.3734e-03]],\n",
            "\n",
            "         [[1.2097e-03]],\n",
            "\n",
            "         [[5.7420e-04]]],\n",
            "\n",
            "\n",
            "        [[[9.1587e-04]],\n",
            "\n",
            "         [[9.3151e-04]],\n",
            "\n",
            "         [[1.2583e-03]],\n",
            "\n",
            "         [[8.9729e-04]],\n",
            "\n",
            "         [[8.2670e-04]],\n",
            "\n",
            "         [[9.6214e-04]],\n",
            "\n",
            "         [[6.3004e-04]],\n",
            "\n",
            "         [[7.0897e-04]],\n",
            "\n",
            "         [[9.3618e-04]],\n",
            "\n",
            "         [[4.4173e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.1935e-04]],\n",
            "\n",
            "         [[1.5001e-04]],\n",
            "\n",
            "         [[8.2558e-04]],\n",
            "\n",
            "         [[1.4199e-04]],\n",
            "\n",
            "         [[2.3938e-04]],\n",
            "\n",
            "         [[4.1783e-04]],\n",
            "\n",
            "         [[2.9500e-04]],\n",
            "\n",
            "         [[4.0463e-04]],\n",
            "\n",
            "         [[3.1904e-04]],\n",
            "\n",
            "         [[4.8267e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.0330e-04]],\n",
            "\n",
            "         [[4.0500e-04]],\n",
            "\n",
            "         [[1.1201e-03]],\n",
            "\n",
            "         [[6.7627e-04]],\n",
            "\n",
            "         [[3.9702e-04]],\n",
            "\n",
            "         [[6.0403e-04]],\n",
            "\n",
            "         [[7.4286e-04]],\n",
            "\n",
            "         [[3.3158e-04]],\n",
            "\n",
            "         [[1.1079e-03]],\n",
            "\n",
            "         [[6.7921e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.7591e-04]],\n",
            "\n",
            "         [[5.6535e-04]],\n",
            "\n",
            "         [[3.4780e-04]],\n",
            "\n",
            "         [[2.6187e-04]],\n",
            "\n",
            "         [[2.3953e-04]],\n",
            "\n",
            "         [[4.8907e-04]],\n",
            "\n",
            "         [[4.9498e-04]],\n",
            "\n",
            "         [[7.0638e-04]],\n",
            "\n",
            "         [[5.8660e-04]],\n",
            "\n",
            "         [[6.4678e-04]]],\n",
            "\n",
            "\n",
            "        [[[9.2160e-04]],\n",
            "\n",
            "         [[4.6431e-04]],\n",
            "\n",
            "         [[7.2845e-04]],\n",
            "\n",
            "         [[8.2054e-04]],\n",
            "\n",
            "         [[4.2234e-04]],\n",
            "\n",
            "         [[2.4571e-04]],\n",
            "\n",
            "         [[7.6740e-04]],\n",
            "\n",
            "         [[8.3434e-04]],\n",
            "\n",
            "         [[9.5754e-04]],\n",
            "\n",
            "         [[5.9532e-04]]],\n",
            "\n",
            "\n",
            "        [[[2.0847e-04]],\n",
            "\n",
            "         [[2.6557e-04]],\n",
            "\n",
            "         [[2.8987e-04]],\n",
            "\n",
            "         [[3.3442e-04]],\n",
            "\n",
            "         [[1.1671e-04]],\n",
            "\n",
            "         [[1.1310e-04]],\n",
            "\n",
            "         [[3.4783e-04]],\n",
            "\n",
            "         [[3.1852e-04]],\n",
            "\n",
            "         [[2.5670e-04]],\n",
            "\n",
            "         [[2.5051e-04]]],\n",
            "\n",
            "\n",
            "        [[[9.6228e-04]],\n",
            "\n",
            "         [[6.0933e-04]],\n",
            "\n",
            "         [[1.5952e-03]],\n",
            "\n",
            "         [[4.0957e-04]],\n",
            "\n",
            "         [[2.9672e-04]],\n",
            "\n",
            "         [[3.2609e-04]],\n",
            "\n",
            "         [[9.5373e-04]],\n",
            "\n",
            "         [[6.6619e-04]],\n",
            "\n",
            "         [[8.8756e-04]],\n",
            "\n",
            "         [[6.3372e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.3251e-04]],\n",
            "\n",
            "         [[4.2349e-04]],\n",
            "\n",
            "         [[5.7729e-04]],\n",
            "\n",
            "         [[1.9484e-04]],\n",
            "\n",
            "         [[6.3198e-04]],\n",
            "\n",
            "         [[3.0358e-04]],\n",
            "\n",
            "         [[5.3940e-04]],\n",
            "\n",
            "         [[3.4775e-04]],\n",
            "\n",
            "         [[7.2607e-04]],\n",
            "\n",
            "         [[5.7773e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.6241e-04]],\n",
            "\n",
            "         [[1.2721e-03]],\n",
            "\n",
            "         [[8.3001e-04]],\n",
            "\n",
            "         [[4.5451e-04]],\n",
            "\n",
            "         [[6.8142e-04]],\n",
            "\n",
            "         [[4.1513e-04]],\n",
            "\n",
            "         [[1.2986e-03]],\n",
            "\n",
            "         [[4.9985e-04]],\n",
            "\n",
            "         [[6.7079e-04]],\n",
            "\n",
            "         [[6.1610e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.5709e-04]],\n",
            "\n",
            "         [[6.6437e-04]],\n",
            "\n",
            "         [[5.7886e-04]],\n",
            "\n",
            "         [[5.5599e-04]],\n",
            "\n",
            "         [[3.3321e-04]],\n",
            "\n",
            "         [[6.9979e-04]],\n",
            "\n",
            "         [[7.3597e-04]],\n",
            "\n",
            "         [[1.0879e-03]],\n",
            "\n",
            "         [[7.3917e-04]],\n",
            "\n",
            "         [[3.6484e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.9295e-04]],\n",
            "\n",
            "         [[6.5246e-04]],\n",
            "\n",
            "         [[6.6887e-04]],\n",
            "\n",
            "         [[4.0273e-04]],\n",
            "\n",
            "         [[3.6896e-04]],\n",
            "\n",
            "         [[3.5850e-04]],\n",
            "\n",
            "         [[3.7298e-04]],\n",
            "\n",
            "         [[4.8471e-04]],\n",
            "\n",
            "         [[3.6667e-04]],\n",
            "\n",
            "         [[3.0144e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.3946e-04]],\n",
            "\n",
            "         [[8.0550e-04]],\n",
            "\n",
            "         [[8.3599e-04]],\n",
            "\n",
            "         [[3.6931e-04]],\n",
            "\n",
            "         [[4.4943e-04]],\n",
            "\n",
            "         [[9.8500e-04]],\n",
            "\n",
            "         [[9.5453e-04]],\n",
            "\n",
            "         [[9.0359e-04]],\n",
            "\n",
            "         [[1.7979e-03]],\n",
            "\n",
            "         [[5.3451e-04]]],\n",
            "\n",
            "\n",
            "        [[[1.0103e-03]],\n",
            "\n",
            "         [[1.0118e-03]],\n",
            "\n",
            "         [[3.8254e-04]],\n",
            "\n",
            "         [[5.0209e-04]],\n",
            "\n",
            "         [[6.8431e-04]],\n",
            "\n",
            "         [[7.8342e-04]],\n",
            "\n",
            "         [[1.0562e-03]],\n",
            "\n",
            "         [[5.8369e-04]],\n",
            "\n",
            "         [[8.1117e-04]],\n",
            "\n",
            "         [[6.7034e-04]]],\n",
            "\n",
            "\n",
            "        [[[7.5690e-04]],\n",
            "\n",
            "         [[1.4412e-03]],\n",
            "\n",
            "         [[9.1471e-04]],\n",
            "\n",
            "         [[1.0231e-03]],\n",
            "\n",
            "         [[6.1085e-04]],\n",
            "\n",
            "         [[9.5738e-04]],\n",
            "\n",
            "         [[7.1244e-04]],\n",
            "\n",
            "         [[4.2330e-04]],\n",
            "\n",
            "         [[6.8148e-04]],\n",
            "\n",
            "         [[3.5542e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.5029e-04]],\n",
            "\n",
            "         [[5.9831e-04]],\n",
            "\n",
            "         [[4.2900e-04]],\n",
            "\n",
            "         [[3.5119e-04]],\n",
            "\n",
            "         [[2.9317e-04]],\n",
            "\n",
            "         [[2.7223e-04]],\n",
            "\n",
            "         [[4.1758e-04]],\n",
            "\n",
            "         [[2.7847e-04]],\n",
            "\n",
            "         [[3.7049e-04]],\n",
            "\n",
            "         [[3.0879e-04]]],\n",
            "\n",
            "\n",
            "        [[[1.1790e-03]],\n",
            "\n",
            "         [[1.6807e-03]],\n",
            "\n",
            "         [[1.8463e-03]],\n",
            "\n",
            "         [[1.1720e-03]],\n",
            "\n",
            "         [[6.5016e-04]],\n",
            "\n",
            "         [[6.0127e-04]],\n",
            "\n",
            "         [[1.4616e-03]],\n",
            "\n",
            "         [[1.2305e-03]],\n",
            "\n",
            "         [[1.3163e-03]],\n",
            "\n",
            "         [[1.3673e-03]]],\n",
            "\n",
            "\n",
            "        [[[6.6536e-04]],\n",
            "\n",
            "         [[1.5873e-03]],\n",
            "\n",
            "         [[5.4605e-04]],\n",
            "\n",
            "         [[1.2533e-03]],\n",
            "\n",
            "         [[6.2810e-04]],\n",
            "\n",
            "         [[3.8328e-04]],\n",
            "\n",
            "         [[9.0053e-04]],\n",
            "\n",
            "         [[7.1246e-04]],\n",
            "\n",
            "         [[1.3381e-03]],\n",
            "\n",
            "         [[6.0033e-04]]],\n",
            "\n",
            "\n",
            "        [[[1.4934e-03]],\n",
            "\n",
            "         [[1.7346e-03]],\n",
            "\n",
            "         [[7.5096e-04]],\n",
            "\n",
            "         [[6.0023e-04]],\n",
            "\n",
            "         [[9.0009e-04]],\n",
            "\n",
            "         [[7.9835e-04]],\n",
            "\n",
            "         [[1.0477e-03]],\n",
            "\n",
            "         [[9.2252e-04]],\n",
            "\n",
            "         [[1.6965e-03]],\n",
            "\n",
            "         [[1.0822e-03]]],\n",
            "\n",
            "\n",
            "        [[[6.1832e-04]],\n",
            "\n",
            "         [[7.5417e-04]],\n",
            "\n",
            "         [[5.8307e-04]],\n",
            "\n",
            "         [[1.2039e-03]],\n",
            "\n",
            "         [[5.3212e-04]],\n",
            "\n",
            "         [[1.2877e-03]],\n",
            "\n",
            "         [[1.0081e-03]],\n",
            "\n",
            "         [[4.1493e-04]],\n",
            "\n",
            "         [[9.3972e-04]],\n",
            "\n",
            "         [[1.0922e-03]]],\n",
            "\n",
            "\n",
            "        [[[5.9450e-04]],\n",
            "\n",
            "         [[1.3335e-03]],\n",
            "\n",
            "         [[9.0019e-04]],\n",
            "\n",
            "         [[1.2709e-03]],\n",
            "\n",
            "         [[6.2409e-04]],\n",
            "\n",
            "         [[7.8843e-04]],\n",
            "\n",
            "         [[1.2312e-03]],\n",
            "\n",
            "         [[7.3443e-04]],\n",
            "\n",
            "         [[7.5818e-04]],\n",
            "\n",
            "         [[7.2864e-04]]],\n",
            "\n",
            "\n",
            "        [[[7.0112e-04]],\n",
            "\n",
            "         [[4.9398e-04]],\n",
            "\n",
            "         [[1.0317e-03]],\n",
            "\n",
            "         [[5.0838e-04]],\n",
            "\n",
            "         [[4.9150e-04]],\n",
            "\n",
            "         [[7.3928e-04]],\n",
            "\n",
            "         [[6.5004e-04]],\n",
            "\n",
            "         [[4.7314e-04]],\n",
            "\n",
            "         [[9.1993e-04]],\n",
            "\n",
            "         [[6.9658e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.8609e-04]],\n",
            "\n",
            "         [[1.1630e-03]],\n",
            "\n",
            "         [[9.2233e-04]],\n",
            "\n",
            "         [[1.0234e-03]],\n",
            "\n",
            "         [[5.1456e-04]],\n",
            "\n",
            "         [[6.4554e-04]],\n",
            "\n",
            "         [[9.5267e-04]],\n",
            "\n",
            "         [[5.2826e-04]],\n",
            "\n",
            "         [[5.7032e-04]],\n",
            "\n",
            "         [[2.8074e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.9949e-04]],\n",
            "\n",
            "         [[8.0821e-04]],\n",
            "\n",
            "         [[4.2904e-04]],\n",
            "\n",
            "         [[7.0559e-04]],\n",
            "\n",
            "         [[4.2283e-04]],\n",
            "\n",
            "         [[6.0834e-04]],\n",
            "\n",
            "         [[1.3113e-03]],\n",
            "\n",
            "         [[7.3560e-04]],\n",
            "\n",
            "         [[6.9004e-04]],\n",
            "\n",
            "         [[4.1395e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.0587e-04]],\n",
            "\n",
            "         [[4.2323e-04]],\n",
            "\n",
            "         [[4.5690e-04]],\n",
            "\n",
            "         [[3.4357e-04]],\n",
            "\n",
            "         [[4.3550e-04]],\n",
            "\n",
            "         [[5.1648e-04]],\n",
            "\n",
            "         [[6.1676e-04]],\n",
            "\n",
            "         [[3.1752e-04]],\n",
            "\n",
            "         [[3.5407e-04]],\n",
            "\n",
            "         [[2.1936e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.9614e-04]],\n",
            "\n",
            "         [[6.7884e-04]],\n",
            "\n",
            "         [[1.9842e-04]],\n",
            "\n",
            "         [[2.9264e-04]],\n",
            "\n",
            "         [[3.4299e-04]],\n",
            "\n",
            "         [[3.5875e-04]],\n",
            "\n",
            "         [[1.1342e-03]],\n",
            "\n",
            "         [[5.1048e-04]],\n",
            "\n",
            "         [[9.1441e-04]],\n",
            "\n",
            "         [[4.8170e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.7647e-04]],\n",
            "\n",
            "         [[1.3043e-03]],\n",
            "\n",
            "         [[1.1621e-03]],\n",
            "\n",
            "         [[8.0975e-04]],\n",
            "\n",
            "         [[5.2048e-04]],\n",
            "\n",
            "         [[2.4761e-04]],\n",
            "\n",
            "         [[1.3675e-03]],\n",
            "\n",
            "         [[7.8847e-04]],\n",
            "\n",
            "         [[1.3355e-03]],\n",
            "\n",
            "         [[1.8210e-03]]],\n",
            "\n",
            "\n",
            "        [[[1.1875e-03]],\n",
            "\n",
            "         [[6.8734e-04]],\n",
            "\n",
            "         [[1.3938e-03]],\n",
            "\n",
            "         [[1.0353e-03]],\n",
            "\n",
            "         [[6.2443e-04]],\n",
            "\n",
            "         [[1.2942e-03]],\n",
            "\n",
            "         [[2.2236e-03]],\n",
            "\n",
            "         [[2.3482e-03]],\n",
            "\n",
            "         [[1.0509e-03]],\n",
            "\n",
            "         [[7.6560e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.0241e-04]],\n",
            "\n",
            "         [[5.0790e-04]],\n",
            "\n",
            "         [[3.8499e-04]],\n",
            "\n",
            "         [[2.7685e-04]],\n",
            "\n",
            "         [[3.1108e-04]],\n",
            "\n",
            "         [[3.0689e-04]],\n",
            "\n",
            "         [[3.4450e-04]],\n",
            "\n",
            "         [[5.2612e-04]],\n",
            "\n",
            "         [[6.3456e-04]],\n",
            "\n",
            "         [[5.9464e-04]]],\n",
            "\n",
            "\n",
            "        [[[3.1604e-04]],\n",
            "\n",
            "         [[2.8484e-04]],\n",
            "\n",
            "         [[1.9545e-04]],\n",
            "\n",
            "         [[5.2190e-04]],\n",
            "\n",
            "         [[3.2645e-04]],\n",
            "\n",
            "         [[2.3570e-04]],\n",
            "\n",
            "         [[2.8607e-04]],\n",
            "\n",
            "         [[2.3992e-04]],\n",
            "\n",
            "         [[3.0668e-04]],\n",
            "\n",
            "         [[3.3391e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.7985e-04]],\n",
            "\n",
            "         [[1.1102e-03]],\n",
            "\n",
            "         [[6.9813e-04]],\n",
            "\n",
            "         [[6.2462e-04]],\n",
            "\n",
            "         [[8.2927e-04]],\n",
            "\n",
            "         [[1.0693e-03]],\n",
            "\n",
            "         [[1.1385e-03]],\n",
            "\n",
            "         [[6.1539e-04]],\n",
            "\n",
            "         [[1.4292e-03]],\n",
            "\n",
            "         [[1.0599e-03]]],\n",
            "\n",
            "\n",
            "        [[[5.6267e-04]],\n",
            "\n",
            "         [[1.1906e-03]],\n",
            "\n",
            "         [[6.8027e-04]],\n",
            "\n",
            "         [[1.0719e-03]],\n",
            "\n",
            "         [[5.0020e-04]],\n",
            "\n",
            "         [[5.3122e-04]],\n",
            "\n",
            "         [[8.0949e-04]],\n",
            "\n",
            "         [[4.7112e-04]],\n",
            "\n",
            "         [[6.1940e-04]],\n",
            "\n",
            "         [[8.7087e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.4193e-04]],\n",
            "\n",
            "         [[3.0682e-04]],\n",
            "\n",
            "         [[5.6540e-04]],\n",
            "\n",
            "         [[2.5016e-04]],\n",
            "\n",
            "         [[4.6734e-04]],\n",
            "\n",
            "         [[4.1445e-04]],\n",
            "\n",
            "         [[4.7288e-04]],\n",
            "\n",
            "         [[4.4592e-04]],\n",
            "\n",
            "         [[6.3593e-04]],\n",
            "\n",
            "         [[5.8821e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.4232e-04]],\n",
            "\n",
            "         [[7.0334e-04]],\n",
            "\n",
            "         [[6.8665e-04]],\n",
            "\n",
            "         [[1.7685e-04]],\n",
            "\n",
            "         [[4.2633e-04]],\n",
            "\n",
            "         [[7.8139e-04]],\n",
            "\n",
            "         [[7.9993e-04]],\n",
            "\n",
            "         [[3.0127e-04]],\n",
            "\n",
            "         [[8.1805e-04]],\n",
            "\n",
            "         [[5.6494e-04]]]], dtype=torch.float64, grad_fn=<SqrtBackward0>)\n",
            "64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-66fc31ab949d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_margin_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtrain_capsnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mtest_capsnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# train(e, capsule_net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-66fc31ab949d>\u001b[0m in \u001b[0;36mtrain_capsnet\u001b[0;34m(model, optimizer, train_loader, epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pytorch-CapsuleNet/capsnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, data, x, target, reconstructions)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmargin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-66fc31ab949d>\u001b[0m in \u001b[0;36mnew_margin_loss\u001b[0;34m(x, labels, size_average)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (26) must match the size of tensor b (10) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}